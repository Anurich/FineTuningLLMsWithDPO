# Fine Tuning LLMs With DPO

This project discusses the complete pipeline, of creating an end-to-end system of finetuning the LLMs, with PEFT STTF and DPO. This project explanation is also available in the medium article which can be accessed through this link https://medium.com/dev-genius/how-to-harness-peft-sftt-and-dpo-to-fine-tune-llms-394e9cd0b150.

# Project Description.
The dataset used in this project is taken from Huggingface.
<ol> 
  <li>https://huggingface.co/datasets/gbharti/finance-alpaca This is used to finetune the SFTT model.</li>
  <li>https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo This is used to finetune the DPO.</li>
</ol>

The Library used in this project are:
<ol>
  <li>Python</li>
  <li>HuggingFace</li>
</ol>

It also contains the file which shows how we can convert the dataset into the DPO format dataset. Read the medium article to get a better understanding of the complete project. If you feel like there are somethings that need to be changed or some advice on how we can improve the current system don't hesitate to write to me. Feel free to utilise the code and create awsome project in the field of NLP.
