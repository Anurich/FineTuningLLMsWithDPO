# Fine Tuning LLMs With DPO

This project discusses the complete pipeline, of creating an end-to-end system of finetuning the LLMs, with PEFT STTF and DPO. This project explanation is also available in the medium article which can be accessed through this link https://medium.com/dev-genius/how-to-harness-peft-sftt-and-dpo-to-fine-tune-llms-394e9cd0b150.

# Project Description.
The dataset used in this project is taken from Huggingface.
<ol> 
  <li>https://huggingface.co/datasets/gbharti/finance-alpaca This is used to finetune the SFTT model.</li>
  <li>https://huggingface.co/datasets/argilla/distilabel-math-preference-dpo This is used to finetune the DPO.</li>
</ol>

The Library used in this project are:
<ol>
  <li>Python</li>
  <li>HuggingFace</li>
</ol>

